name: Pipeline Eval

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
    paths:
      - "prompts/**"
      - "pipeline_config.yaml"
      - "src/pipeline/**"
      - "src/**"
      - "tests/**"
      - "golden_dataset/**"
      - "promptfoo.config.yaml"
      - "experiment_configs/**"

jobs:
  lint-and-typecheck:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install -e ".[dev]"

      - name: Lint
        run: ruff check src tests

      - name: Type check
        run: mypy src

  unit-tests:
    runs-on: ubuntu-latest
    needs: lint-and-typecheck
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install -e ".[dev]"
          python -m spacy download en_core_web_sm

      - name: Run unit tests
        run: pytest tests/unit -v --tb=short --cov=src --cov-report=xml

      - name: Upload coverage
        uses: actions/upload-artifact@v4
        with:
          name: coverage
          path: coverage.xml

  eval-regression:
    runs-on: ubuntu-latest
    needs: unit-tests
    if: github.event_name == 'pull_request'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install -e ".[dev,eval]"

      - name: Run eval tests
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        run: pytest tests/eval -v --tb=short -m eval

      - name: Run faithfulness eval (DeepEval)
        if: env.OPENROUTER_API_KEY != ''
        env:
          # conftest.py auto-bridges OPENROUTER_API_KEY → OPENAI_API_KEY + OPENAI_BASE_URL
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        run: pytest tests/eval/test_faithfulness.py -v --tb=short

      - name: Check regression
        run: python scripts/check_regression.py --max-regression-pct 2

      - name: Upload eval results
        uses: actions/upload-artifact@v4
        with:
          name: eval-results
          path: eval_results/

  promptfoo-eval:
    runs-on: ubuntu-latest
    needs: unit-tests
    if: github.event_name == 'pull_request'
    steps:
      - uses: actions/checkout@v4

      - name: Set up Node
        uses: actions/setup-node@v4
        with:
          node-version: "20"

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Python dependencies
        run: pip install -e ".[dev]"

      - name: Install promptfoo
        run: npm install -g promptfoo

      - name: Run Promptfoo eval
        if: env.OPENROUTER_API_KEY != ''
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        run: |
          mkdir -p eval_results
          promptfoo eval --config promptfoo.config.yaml --output eval_results/promptfoo_output.json || true

      - name: Skip message (no API key)
        if: env.OPENROUTER_API_KEY == ''
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        run: echo "Skipping Promptfoo eval — OPENROUTER_API_KEY not set"

      - name: Check Promptfoo regression
        if: env.OPENROUTER_API_KEY != ''
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        run: python scripts/check_regression.py --max-regression-pct 2 --promptfoo-results eval_results/promptfoo_output.json

      - name: Upload Promptfoo results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: promptfoo-results
          path: eval_results/promptfoo_output.json
          if-no-files-found: ignore
